Metadata-Version: 1.1
Name: keras-pos-embd
Version: 0.4.0
Summary: Position embedding layers in Keras
Home-page: https://github.com/CyberZHG/keras-pos-embd
Author: CyberZHG
Author-email: CyberZHG@gmail.com
License: MIT
Description: 
        Keras Position Embedding
        ========================
        
        
        .. image:: https://travis-ci.org/CyberZHG/keras-pos-embd.svg
           :target: https://travis-ci.org/CyberZHG/keras-pos-embd
           :alt: Travis
        
        
        .. image:: https://coveralls.io/repos/github/CyberZHG/keras-pos-embd/badge.svg?branch=master
           :target: https://coveralls.io/github/CyberZHG/keras-pos-embd
           :alt: Coverage
        
        
        Position embedding layers in Keras.
        
        Install
        -------
        
        .. code-block:: bash
        
           pip install keras-pos-embd
        
        Usage
        -----
        
        Trainable Embedding
        ^^^^^^^^^^^^^^^^^^^
        
        .. code-block:: python
        
           import keras
           from keras_pos_embd import PositionEmbedding
        
           model = keras.models.Sequential()
           model.add(PositionEmbedding(
               input_shape=(None,),
               input_dim=10,     # The maximum absolute value of positions.
               output_dim=2,     # The dimension of embeddings.
               mask_zero=10000,  # The index that presents padding (because `0` will be used in relative positioning).
               name='Pos-Embd',
           ))
           model.compile('adam', keras.losses.mae, {})
           model.summary()
        
        (Note that you don't need to enable ``mask_zero`` if you would concatenate other layers like word embeddings with masks)
        
        Sin & Cos Embedding
        ^^^^^^^^^^^^^^^^^^^
        
        The `sine and cosine embedding <https://arxiv.org/pdf/1706.03762>`_ has no trainable weights. The layer has two modes, it works just like ``PositionEmbedding`` in ``expand`` mode:
        
        .. code-block:: python
        
           import keras
           from keras_pos_embd import TrigPosEmbedding
        
           model = keras.models.Sequential()
           model.add(TrigPosEmbedding(
               input_shape=(None,),
               output_dim=30,                      # The dimension of embeddings.
               mode=TrigPosEmbedding.MODE_EXPAND,  # Use `expand` mode
               name='Pos-Embd',
           ))
           model.compile('adam', keras.losses.mae, {})
           model.summary()
        
        If you want to add this embedding to existed embedding, then there is no need to add a position input in ``add`` mode:
        
        .. code-block:: python
        
           import keras
           from keras_pos_embd import TrigPosEmbedding
        
           model = keras.models.Sequential()
           model.add(TrigPosEmbedding(
               input_shape=(None, 100),
               mode=TrigPosEmbedding.MODE_ADD,  # Use `add` mode (default)
               name='Pos-Embd',
           ))
           model.compile('adam', keras.losses.mae, {})
           model.summary()
        
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 2.7
Classifier: Programming Language :: Python :: 3.6
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
