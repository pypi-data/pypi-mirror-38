{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Basic Analysis Example\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, print_function, division\n\n# Authors: Tam\u00e1s G\u00e1l <tgal@km3net.de>, Moritz Lotze <mlotze@km3net.de>\n# License: BSD-3\n# Date: 2017-10-10\n# Status: Under construction..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparation\n-----------\nThe very first thing we do is importing our libraries and setting up\nthe Jupyter Notebook environment.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt    # our plotting module\nimport pandas as pd    # the main HDF5 reader\nimport numpy as np    # must have\nimport km3pipe as kp    # some KM3NeT related helper functions\nimport seaborn as sns    # beautiful statistical plots!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "this is just to make our plots a bit \"nicer\", you can skip it\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import km3pipe.style\nkm3pipe.style.use(\"km3pipe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accessing the Data File(s)\n--------------------------\nIn the following, we will work with one random simulation file with\nreconstruction information from JGandalf which has been converted\nfrom ROOT to HDF5 using the ``tohdf5`` command line tool provided by\n``KM3Pipe``.\n\nYou can find the documentation here:\nhttp://km3pipe.readthedocs.io/en/latest/cmd.html#tohdf\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note for Lyon Users\n~~~~~~~~~~~~~~~~~~~\nIf you are working on the Lyon cluster, you can activate the latest KM3Pipe\nwith the following command (put it in your ``~/.bashrc`` to load it\nautomatically in each shell session)::\n\n    source $KM3NET_THRONG_DIR/src/python/pyenv.sh\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Converting from ROOT to HDF5 (if needed)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nChoose a file (take e.g. one from /in2p3/km3net/mc/...),\nload the appropriate Jpp/Aanet version and convert it via::\n\n    tohdf5 --aa-format=the_file.root --ignore-hits --skip-header\n\nNote that you may have to ``--skip-header`` otherwise you might\nencounter some segfaults. There is currently a big mess of different\nversions of libraries in several levels of the MC file processings.\n\nThe ``--ignore-hits`` will skip the hit information, so the converted file\nis much smaller (normally around 2-3 MBytes). Skip this option if you want\nto read the hit information too. The file will still be smaller than the\nROOT file (about 1/3).\n\nLuckily, a handful people are preparing the HDF5 conversion, so in future\nyou can download them directly, without thinking about which Jpp or Aanet\nversion you need to open them.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First Look at the Data\n----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filepath = \"data/basic_analysis_sample.h5\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can have a quick look at the file with the ``ptdump`` command\nin the terminal::\n\n    ptdump filename.h5\n\nFor further information, check out the documentation of the KM3NeT HDF5\nformat definition: http://km3pipe.readthedocs.io/en/latest/hdf5.html\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``/event_info`` table contains general information about each event.\nThe data is a simple 2D table and each event is represented by a single row.\n\nLet's have a look at the first few rows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "event_info = pd.read_hdf(filepath, '/event_info')\nprint(event_info.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will read out the MC tracks which are stored under ``/mc_tracks``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tracks = pd.read_hdf(filepath, '/mc_tracks')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "also read event info, for things like weights\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "info = pd.read_hdf(filepath, '/event_info')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It has a similar structure, but now you can have multiple rows which belong\nto an event. The ``event_id`` column holds the ID of the corresponding event.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tracks.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now are accessing the first track for each event by grouping via\n``event_id`` and calling the ``first()`` method of the\n``Pandas.DataFrame`` object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "primaries = tracks.groupby('event_id').first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are the first 5 primaries:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(primaries.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating some Fancy Graphs\n--------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "primaries.energy.hist(bins=100, log=True)\nplt.xlabel('energy [GeV]')\nplt.ylabel('number of events')\nplt.title('Energy Distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "primaries.bjorkeny.hist(bins=100)\nplt.xlabel('bjorken-y')\nplt.ylabel('number of events')\nplt.title('bjorken-y Distribution')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "zeniths = kp.math.zenith(primaries.filter(regex='^dir_.?$'))\nprimaries['zenith'] = zeniths\n\nplt.hist(np.cos(primaries.zenith), bins=21, histtype='step', linewidth=2)\nplt.xlabel(r'cos($\\theta$)')\nplt.ylabel('number of events')\nplt.title('Zenith Distribution')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Starting positions of primaries\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.hist2d(primaries.pos_x, primaries.pos_y, bins=100, cmap='viridis')\nplt.xlabel('x [m]')\nplt.ylabel('y [m]')\nplt.title('2D Plane')\nplt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you have seaborn installed (`pip install seaborn`), you can easily create\nnice jointplots:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    import seaborn as sns    # noqa\n    km3pipe.style.use(\"km3pipe\")    # reset matplotlib style\nexcept:\n    print(\"No seaborn found, skipping example.\")\nelse:\n    g = sns.jointplot('pos_x', 'pos_y', data=primaries, kind='hex')\n    g.set_axis_labels(\"x [m]\", \"y[m]\")\n    plt.subplots_adjust(right=0.90)    # make room for the colorbar\n    plt.title(\"2D Plane\")\n    plt.colorbar()\n    plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D    # noqa\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter3D(primaries.pos_x, primaries.pos_y, primaries.pos_z, s=3)\nax.set_xlabel('x [m]', labelpad=10)\nax.set_ylabel('y [m]', labelpad=10)\nax.set_zlabel('z [m]', labelpad=10)\nax.set_title('3D Plane')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gandalfs = pd.read_hdf(filepath, '/reco/gandalf')\nprint(gandalfs.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gandalfs.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.hist(gandalfs['lambda'], bins=50, log=True)\nplt.xlabel('lambda parameter')\nplt.ylabel('count')\nplt.title('Lambda Distribution of Reconstructed Events')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gandalfs['zenith'] = kp.math.zenith(gandalfs.filter(regex='^dir_.?$'))\n\nplt.hist((gandalfs.zenith - primaries.zenith).dropna(), bins=100)\nplt.xlabel(r'true zenith - reconstructed zenith [rad]')\nplt.ylabel('count')\nplt.title('Zenith Reconstruction Difference')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "l = 0.2\nlambda_cut = gandalfs['lambda'] < l\nplt.hist((gandalfs.zenith - primaries.zenith)[lambda_cut].dropna(), bins=100)\nplt.xlabel(r'true zenith - reconstructed zenith [rad]')\nplt.ylabel('count')\nplt.title('Zenith Reconstruction Difference for lambda < {}'.format(l))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Combined zenith reco plot for different lambda cuts\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\nfor l in [100, 5, 2, 1, 0.1]:\n    l_cut = gandalfs['lambda'] < l\n    ax.hist((primaries.zenith - gandalfs.zenith)[l_cut].dropna(),\n            bins=100,\n            label=r\"$\\lambda$ = {}\".format(l),\n            alpha=.7)\nplt.xlabel(r'true zenith - reconstructed zenith [rad]')\nplt.ylabel('count')\nplt.legend()\nplt.title('Zenith Reconstruction Difference for some Lambda Cuts')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fitting Angular resolutions\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nLet's fit some distributions: gaussian + lorentz (aka norm + cauchy)\n\nFitting the gaussian to the whole range is a very bad fit, so\nwe make a second gaussian fit only to +- 10 degree.\nConversely, the Cauchy (lorentz) distribution is a near perfect fit\n(note that ``2 gamma = FWHM``).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.stats import cauchy, norm    # noqa\n\nresiduals = gandalfs.zenith - primaries.zenith\ncut = (gandalfs['lambda'] < l) & (np.abs(residuals) < 2 * np.pi)\nresiduals = residuals[cut]\ninfo[cut]\n\n# convert rad -> deg\nresiduals = residuals * 180 / np.pi\n\npi = 180\n# x axis for plotting\nx = np.linspace(-pi, pi, 1000)\n\nc_loc, c_gamma = cauchy.fit(residuals)\nfwhm = 2 * c_gamma\n\ng_mu_bad, g_sigma_bad = norm.fit(residuals)\ng_mu, g_sigma = norm.fit(residuals[np.abs(residuals) < 10])\n\nplt.hist(residuals, bins='auto', label='Histogram', normed=True, alpha=.7)\nplt.plot(\n    x,\n    cauchy(c_loc, c_gamma).pdf(x),\n    label='Lorentz: FWHM $=${:.3f}'.format(fwhm),\n    linewidth=2\n)\nplt.plot(\n    x,\n    norm(g_mu_bad, g_sigma_bad).pdf(x),\n    label='Unrestricted Gauss: $\\sigma =$ {:.3f}'.format(g_sigma_bad),\n    linewidth=2\n)\nplt.plot(\n    x,\n    norm(g_mu, g_sigma).pdf(x),\n    label='+- 10 deg Gauss: $\\sigma =$ {:.3f}'.format(g_sigma),\n    linewidth=2\n)\nplt.xlim(-pi / 4, pi / 4)\nplt.xlabel('Zenith residuals / deg')\nplt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also look at the median resolution without doing any fits.\n\nIn textbooks, this metric is also called Median Absolute Deviation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "resid_median = np.median(residuals)\nresiduals_shifted_by_median = residuals - resid_median\nabsolute_deviation = np.abs(residuals_shifted_by_median)\nresid_mad = np.median(absolute_deviation)\n\nplt.hist(np.abs(residuals), alpha=.7, bins='auto', label='Absolute residuals')\nplt.axvline(resid_mad, label='MAD: {:.2f}'.format(resid_mad), linewidth=3)\nplt.title(\"Average resolution: {:.3f} degree\".format(resid_mad))\nplt.legend()\nplt.xlabel('Absolute zenith residuals / deg')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}