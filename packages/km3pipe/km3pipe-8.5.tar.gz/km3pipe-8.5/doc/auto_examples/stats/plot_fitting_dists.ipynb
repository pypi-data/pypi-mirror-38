{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Fitting Distributions\n\n\nHistograms, PDF fits, Kernel Density.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, print_function, division\n\n# Author: Moritz Lotze <mlotze@km3net.de>\n# License: BSD-3\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\n\nfrom scipy.stats import norm\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KernelDensity\n\nimport km3pipe.style.moritz    # noqa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First generate some pseudodata: A bimodal gaussian, + noise.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N = 100\nbmg = np.concatenate((\n    np.random.normal(15, 1, int(0.3 * N)),\n    np.random.normal(20, 1, int(0.7 * N))\n))\nnoise_bmg = 0.5\ndata = np.random.normal(bmg, noise_bmg)[:, np.newaxis]\n\n# make X axis for plots\nx = np.linspace(5, 35, 3 * N + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Histograms (nonparametric)\n--------------------------\n\nThe simplest nonparametric density estimation tool is the Histogram.\nChoosing the binning manually can be tedious, however:\n\n15 bins, spaced from ``data.min()`` to ``data.max()``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.hist(data, bins=15, alpha=.5, normed=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Auto Binning (recommended)\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nUse builtin (numpy's) heuristics to figure out best binning.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.hist(data, bins='auto', alpha=.5, normed=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bayesian Blocks\n^^^^^^^^^^^^^^^\n\nTODO: Compute optimal segmentation of data with Scargle\u2019s Bayesian Blocks.\nProduces bins of uneven width.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit Distribution via Maximum Likelihood\n---------------------------------------\n\nIf we have a hypothesis what the distribution looks like (e.g. gaussian),\nand want to fit its parameters.\n\nThe nice thing is, you can define your own PDFs in scipy and fit it.\nOr take one from the dozens of pre-defined ones.\n\nHowever, there is no *bimodal* gaussian implemented in scipy yet :/\nIn this case, either define it yourself, or use a GMM (below)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mu, sig = norm.fit(data)\n\nplt.fill_between(x, norm(mu, sig).pdf(x), alpha=.5, label='Fitted')\nplt.legend()\nprint('Unimodal Gaussian Fit:  Mean {:.4}, stdev {:.4}'.format(mu, sig))\nplt.hist(data, bins='auto', alpha=.3, normed=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, the result is rather silly, since we are only fitting *one*\nof the two gaussians.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit Gaussian Mixture Model (GMM)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAssuming the data is the sum of one or more gaussians.\nEasily handles multidimensional case as well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gmm = GaussianMixture(n_components=2, covariance_type='spherical')\ngmm.fit(data)\n\nmu1 = gmm.means_[0, 0]\nmu2 = gmm.means_[1, 0]\nvar1, var2 = gmm.covariances_\nwgt1, wgt2 = gmm.weights_\nprint(\n    '''Fit:\n      1: Mean {:.4}, var {:.4}, weight {:.4}\n      2: Mean {:.4}, var {:.4}, weight {:.4}\n'''.format(mu1, var1, wgt1, mu2, var2, wgt2)\n)\n\nplt.hist(data, bins='auto', alpha=.3, normed=True)\nplt.vlines((mu1, mu2), ymin=0, ymax=0.35, label='Fitted Means')\nplt.plot(x, norm.pdf(x, mu1, np.sqrt(var1)))\nplt.plot(x, norm.pdf(x, mu2, np.sqrt(var2)))\nplt.legend()\nplt.title('Gaussian Mixture Model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Kernel Density: (non-parametric)\n--------------------------------\n\nIf we have no strong assumptions about the underlying pdf.\n\n\"Smooth out\" each event with a kernel (e.g. gaussian) of\na certain bandwidth, then add together all these mini-functions.\n\nThe \"bandwidth\" (width of the kernel function) depends on the data, and\ncan be estimated using cross-validation + maximum likelihood\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "in Statsmodels\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dens = sm.nonparametric.KDEUnivariate(data)\ndens.fit()\n\nkde_sm = dens.evaluate(x)\nplt.fill_between(x, kde_sm, alpha=.5, label='KDE')\nplt.hist(data, bins='auto', alpha=.3, normed=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "in scikit-learn\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params = {'bandwidth': np.logspace(-2, 2, 50)}\ngrid = GridSearchCV(KernelDensity(), params)\ngrid.fit(data)\n\nprint(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n\n# use the best estimator to compute the kernel density estimate\nkde_best = grid.best_estimator_\nkde_sk = np.exp(kde_best.score_samples(x[:, np.newaxis]))\nplt.fill_between(x, kde_sk, alpha=.5, label='KDE')\nplt.hist(data, bins='auto', alpha=.3, normed=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "References\n----------\n\n- B.W. Silverman, \u201cDensity Estimation for Statistics and Data Analysis\u201d\n- Hastie, Tibshirani and Friedman,\n  \u201cThe Elements of Statistical Learning: Data Mining, Inference,\n  and Prediction\u201d, Springer (2009)\n- Liu, R., Yang, L.\n  \u201cKernel estimation of multivariate cumulative distribution function.\u201d\n  Journal of Nonparametric Statistics (2008)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}