
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=cp1252" />
    <title>Stochastic Gradient Descent</title>
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/style.css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="next" title="Load Model" href="loadmodel.html" />
    <link rel="prev" title="Neural Network" href="neuralnetwork.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="stochastic-gradient-descent">
<h1>Stochastic Gradient Descent</h1>
<p>Minimize an objective function using a stochastic approximation of gradient descent.</p>
<dl class="docutils">
<dt>Inputs</dt>
<dd><dl class="first last docutils">
<dt>Data</dt>
<dd>input dataset</dd>
<dt>Preprocessor</dt>
<dd>preprocessing method(s)</dd>
</dl>
</dd>
<dt>Outputs</dt>
<dd><dl class="first last docutils">
<dt>Learner</dt>
<dd>stochastic gradient descent learning algorithm</dd>
<dt>Model</dt>
<dd>trained model</dd>
</dl>
</dd>
</dl>
<p>The <strong>Stochastic Gradient Descent</strong> widget uses <a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank">stochastic gradient descent</a> that minimizes a chosen loss function with a linear function. The algorithm approximates a true gradient by considering one sample at a time, and simultaneously updates the model based on the gradient of the loss function. For regression, it returns predictors as minimizers of the sum, i.e. M-estimators, and is especially useful for large-scale and sparse datasets.</p>
<div class="figure">
<a class="reference internal image-reference" href="../../_images/StochasticGradientDescent-stamped.png"><img alt="../../_images/StochasticGradientDescent-stamped.png" src="../../_images/StochasticGradientDescent-stamped.png" style="width: 383.0px; height: 671.0px;" /></a>
</div>
<ol class="arabic">
<li><p class="first">Specify the name of the model. The default name is “SGD”.</p>
</li>
<li><p class="first">Algorithm parameters. Classification loss function:</p>
<ul class="simple">
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Hinge_loss" target="_blank">Hinge</a> (linear SVM)</li>
<li><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" target="_blank">Logistic Regression</a> (logistic regression SGD)</li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Huber_loss" target="_blank">Modified Huber</a> (smooth loss that brings tolerance to outliers as well as probability estimates)</li>
<li><em>Squared Hinge</em> (quadratically penalized hinge)</li>
<li><a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron" target="_blank">Perceptron</a> (linear loss used by the perceptron algorithm)</li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Mean_squared_error#Regression" target="_blank">Squared Loss</a>
(fitted to ordinary least-squares)</li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Huber_loss" target="_blank">Huber</a> (switches to
linear loss beyond &#949;)</li>
<li><a class="reference external" href="http://kernelsvm.tripod.com/" target="_blank">Epsilon insensitive</a> (ignores
errors within &#949;, linear beyond it)</li>
<li><em>Squared epsilon insensitive</em> (loss is squared beyond &#949;-region).</li>
</ul>
<p>Regression loss function:</p>
<ul class="simple">
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Mean_squared_error#Regression" target="_blank">Squared Loss</a>
(fitted to ordinary least-squares)</li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Huber_loss" target="_blank">Huber</a> (switches to
linear loss beyond &#949;)</li>
<li><a class="reference external" href="http://kernelsvm.tripod.com/" target="_blank">Epsilon insensitive</a> (ignores
errors within &#949;, linear beyond it)</li>
<li><em>Squared epsilon insensitive</em> (loss is squared beyond &#949;-region).</li>
</ul>
</li>
<li><p class="first">Regularization norms to prevent overfitting:</p>
<ul class="simple">
<li>None.</li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Taxicab_geometry" target="_blank">Lasso (L1)</a> (L1,
leading to sparse solutions)</li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Norm_(mathematics)#p-norm" target="_blank">Ridge (L2)</a>
(L2, standard regularizer)</li>
<li><a class="reference external" href="https://en.wikipedia.org/wiki/Elastic_net_regularization" target="_blank">Elastic net</a>
(mixing both penalty norms).</li>
</ul>
<p>Regularization strength defines how much regularization will be applied (the less we regularize, the more we allow the model to fit the data) and the mixing parameter what the ratio between L1 and L2 loss will be (if set to 0 then the loss is L2, if set to 1 then it is L1).</p>
</li>
<li><p class="first">Learning parameters.</p>
<ul>
<li><p class="first">Learning rate:</p>
<blockquote>
<div><ul class="simple">
<li><em>Constant</em>: learning rate stays the same through all epochs (passes)</li>
<li><a class="reference external" href="http://leon.bottou.org/projects/sgd" target="_blank">Optimal</a>: a heuristic proposed by Leon Bottou</li>
<li><a class="reference external" href="http://users.ics.aalto.fi/jhollmen/dippa/node22.html" target="_blank">Inverse scaling</a>: earning rate is inversely related to the number of iterations</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Initial learning rate.</p>
</li>
<li><p class="first">Inverse scaling exponent: learning rate decay.</p>
</li>
<li><p class="first">Number of iterations: the number of passes through the training data.</p>
</li>
<li><p class="first">If <em>Shuffle data after each iteration</em> is on, the order of data instances is mixed after each pass.</p>
</li>
<li><p class="first">If <em>Fixed seed for random shuffling</em> is on, the algorithm will use a fixed random seed and enable replicating the results.</p>
</li>
</ul>
</li>
</ol>
<ol class="arabic simple" start="7">
<li>Produce a report.</li>
<li>Press <em>Apply</em> to commit changes. Alternatively, tick the box on the left side of the <em>Apply</em> button and changes will be communicated automatically.</li>
</ol>
<div class="section" id="examples">
<h2>Examples</h2>
<p>For the classification task, we will use <em>iris</em> dataset and test two models on it. We connected <a class="reference internal" href="#"><span class="doc">Stochastic Gradient Descent</span></a> and <a class="reference internal" href="tree.html"><span class="doc">Tree</span></a> to <a class="reference internal" href="../evaluation/testandscore.html"><span class="doc">Test &amp; Score</span></a>. We also connected <a class="reference internal" href="../data/file.html"><span class="doc">File</span></a> to <strong>Test &amp; Score</strong> and observed model performance in the widget.</p>
<div class="figure">
<img alt="../../_images/StochasticGradientDescent-classification.png" src="../../_images/StochasticGradientDescent-classification.png" />
</div>
<p>For the regression task, we will compare three different models to see which predict what kind of results. For the purpose of this example, the <em>housing</em> dataset is used. We connect the <a class="reference internal" href="../data/file.html"><span class="doc">File</span></a> widget to <strong>Stochastic Gradient Descent</strong>, <a class="reference internal" href="linearregression.html"><span class="doc">Linear Regression</span></a> and <a class="reference internal" href="knn.html"><span class="doc">kNN</span></a> widget and all four to the <a class="reference internal" href="../evaluation/predictions.html"><span class="doc">Predictions</span></a> widget.</p>
<div class="figure">
<img alt="../../_images/StochasticGradientDescent-regression.png" src="../../_images/StochasticGradientDescent-regression.png" />
</div>
</div>
</div>


          </div>
          
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2015, Orange Data Mining.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.11</a>
      
      |
      <a href="../../_sources/widgets/model/stochasticgradient.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>